{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import sklearn.linear_model as sklm\n",
    "# import sklearn.model_selection as skms\n",
    "# import sklearn.feature_selection\n",
    "import sklearn.pipeline\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our custom functions\n",
    "from load_data import load_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and validation data:\n",
    "\n",
    "x data is an array of N*784 pixels (N = 2102 for tr, 600 for va)\n",
    "\n",
    "y is a dataframe of index, class_name and class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of train_x.csv: arr of shape (2102, 784)\n",
      "Contents of valid_x.csv: arr of shape (600, 784)\n"
     ]
    }
   ],
   "source": [
    "x_tr, y_tr_df = load_data('x_train.csv', 'y_train.csv')\n",
    "x_va, y_va_df = load_data('x_valid.csv', 'y_valid.csv')\n",
    "x_te = load_data('x_test.csv', 'y_valid.csv')[0]\n",
    "\n",
    "\n",
    "for label, arr in [('train', x_tr), ('valid', x_va)]:\n",
    "    print(\"Contents of %s_x.csv: arr of shape %s\" % (\n",
    "        label, str(arr.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a random image from the validation data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prng = np.random.RandomState(0)\n",
    "prng = np.random.RandomState()\n",
    "N = 3 # num examples of each class to show\n",
    "fig, axgrid = plt.subplots(N, 6, figsize=(6*3, N*2.5))\n",
    "\n",
    "for ll, label in enumerate(['dress', 'pullover', 'top', 'trouser', 'sandal', 'sneaker']):\n",
    "    match_df = y_va_df.query(\"class_name == '%s'\" % label)\n",
    "    match_ids_N = prng.choice(match_df.index, size=N)        \n",
    "    for ii, row_id in enumerate(match_ids_N):\n",
    "        ax = axgrid[ii, ll]\n",
    "        x_SS = x_va[row_id].reshape((28,28))\n",
    "        ax.imshow(x_SS, vmin=0, vmax=255, cmap='gray')\n",
    "        ax.set_xticks([]); ax.set_yticks([]);\n",
    "        if ii == 0:\n",
    "            ax.set_title(label, fontsize=16)\n",
    "plt.subplots_adjust(left=0.01, right=0.99, wspace=.2, hspace=.01)\n",
    "plt.tight_layout();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class distribution:\n",
      "sandal      800\n",
      "sneaker     800\n",
      "dress       400\n",
      "pullover    100\n",
      "top           1\n",
      "trouser       1\n",
      "Name: class_name, dtype: int64\n",
      "Validation class distribution:\n",
      "dress       100\n",
      "trouser     100\n",
      "sandal      100\n",
      "top         100\n",
      "pullover    100\n",
      "sneaker     100\n",
      "Name: class_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tr_class_dist = y_tr_df['class_name'].value_counts()\n",
    "val_class_dist = y_va_df['class_name'].value_counts()\n",
    "\n",
    "print('Training class distribution:')\n",
    "print(tr_class_dist)\n",
    "print('Validation class distribution:')\n",
    "print(val_class_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is heavily skewed with data from sandals and trainers, with only one training image for 2 classes. A challenge will be gaining a balanced weighting for each of the classes such that the dominant classes aren't always favoured by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run search with predefined split s.t. validation set is used for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape: (2102, 784)\n",
      "Validation X shape: (600, 784)\n",
      "Combined X shape: (2702, 784)\n",
      "\n",
      "Training Y shape: (2102, 2)\n",
      "Validation Y shape: (600, 2)\n",
      "Combined Y shape: (2702, 2)\n",
      "\n",
      "Splitter dimensions: 2702\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation datasets\n",
    "x_all = np.vstack((x_tr,x_va))\n",
    "y_all_df = pd.concat([y_tr_df,y_va_df])\n",
    "\n",
    "print(\"Training X shape: %s\\nValidation X shape: %s\\nCombined X shape: %s\\n\" % (x_tr.shape, x_va.shape, x_all.shape))\n",
    "print(\"Training Y shape: %s\\nValidation Y shape: %s\\nCombined Y shape: %s\\n\" % (y_tr_df.shape, y_va_df.shape, y_all_df.shape))\n",
    "\n",
    "valid_indicators = np.hstack([\n",
    "    -1 * np.ones(y_tr_df.shape[0]), # -1 means never include this example in any test split\n",
    "    0  * np.ones(y_va_df.shape[0]), #  0 means include in the first test split (we count starting at 0 in python)\n",
    "    ])\n",
    "\n",
    "# Define custom splitter to use only the validation dataset for hyperparameter selection\n",
    "print(\"Splitter dimensions: %i\" % (valid_indicators.shape[0]))\n",
    "my_splitter = sklearn.model_selection.PredefinedSplit(valid_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = dict(activation=['relu', 'logistic', 'identity', 'tanh'], learning_rate_init=np.logspace(-5, 5, 100), learning_rate = ['constant','adaptive'], hidden_layer_sizes=[(20,),(50,),(100,),(200,),(500,)])\n",
    "\n",
    "fashion_pipes = sklearn.pipeline.Pipeline([\n",
    "    ('rand_search', RandomizedSearchCV(sklearn.neural_network.MLPClassifier( solver='lbfgs', random_state = 0, shuffle=True, early_stopping = True), param_dist, scoring='balanced_accuracy', error_score='raise', random_state=0, return_train_score=True, n_iter=100, cv= my_splitter, n_jobs = -1, refit=False))\n",
    "])\n",
    "\n",
    "fashion_pipes.fit(x_all, y_all_df['class_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridRes = fashion_pipes['rand_search'].cv_results_\n",
    "bestIdx = fashion_pipes['rand_search'].best_index_ # idx 12, 16, 44 all scored the same\n",
    "\n",
    "bestIdxs = [12,16,44]\n",
    "bestParams = dict()\n",
    "bestParams[0] = {k:v[bestIdxs[0]] for k,v in gridRes.items()}\n",
    "bestParams[1] = {k:v[bestIdxs[1]] for k,v in gridRes.items()}\n",
    "bestParams[2] = {k:v[bestIdxs[2]] for k,v in gridRes.items()}\n",
    "\n",
    "display(bestParams)\n",
    "# print(gridRes[\"mean_test_score\"])\n",
    "# display(gridRes[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMLP1b = sklearn.neural_network.MLPClassifier( solver='lbfgs', random_state = 0, shuffle=True, early_stopping = True, learning_rate_init=1e-05,learning_rate ='constant',hidden_layer_sizes=(500,),activation='identity')\n",
    "bestMLP1b.fit(x_tr,y_tr_df[\"class_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc = sklearn.metrics.balanced_accuracy_score(y_tr_df['class_name'], bestMLP1b.predict(x_tr))\n",
    "va_acc = sklearn.metrics.balanced_accuracy_score(y_va_df['class_name'], bestMLP1b.predict(x_va))\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = bestMLP1b.predict(x_tr)\n",
    "pred_va = bestMLP1b.predict(x_va)\n",
    "pred_te = bestMLP1b.predict(x_te)\n",
    "\n",
    "# Save output of prediction on test data to a file.\n",
    "np.savetxt('yhat_test.txt', pred_te, delimiter='\\n', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test data\n",
    "rows = 3\n",
    "cols = 7\n",
    "fig, axgrid = plt.subplots(rows, cols, layout=\"constrained\")\n",
    "\n",
    "for imageID in range(rows*cols):\n",
    "    ax = axgrid[np.unravel_index( imageID, (rows,cols))]\n",
    "    x_SS = x_te[imageID].reshape((28,28))\n",
    "    ax.imshow(x_SS, vmin=0, vmax=255, cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    label = pred_te[imageID]\n",
    "    ax.set_title(label, fontsize=16)\n",
    "# plt.tight_layout();\n",
    "fig.suptitle(\"Sample of test data predictions by model 1B\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO refine search using gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When refit is true we can use the best_estimator_ method, but this doesn't work with refit=false - here we need to manually run a fit on our training set.\n",
    "\n",
    "# best_est_1 = fashion_pipes['rand_search'].best_estimator_\n",
    "\n",
    "# pred_tr = best_est_1.predict(x_tr)\n",
    "# pred_va = best_est_1.predict(x_va)\n",
    "# pred_te = best_est_1.predict(x_te)\n",
    "\n",
    "# # Save output of prediction on test data to a file.\n",
    "# np.savetxt('yhat_test.txt', pred_te, delimiter='\\n', fmt='%s')\n",
    "# sklearn.neural_network.MLPClassifier( solver='lbfgs', random_state = 0, shuffle=True, early_stopping = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for the time being - this was done with refit = true, which fits the final model from randomizedSearchCV onto the entire dataset (tr+val).\n",
    "\n",
    "![Balanced acc of 1.0 or tr and val](best_estimator_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat with data normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data - necessary\n",
    "x_tr_norm = x_tr /255\n",
    "x_va_norm = x_va /255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and validation datasets\n",
    "x_all_norm = np.vstack((x_tr_norm,x_va_norm))\n",
    "y_all_df = pd.concat([y_tr_df,y_va_df])\n",
    "\n",
    "print(\"Training X shape: %s\\nValidation X shape: %s\\nCombined X shape: %s\\n\" % (x_tr_norm.shape, x_va_norm.shape, x_all_norm.shape))\n",
    "print(\"Training Y shape: %s\\nValidation Y shape: %s\\nCombined Y shape: %s\\n\" % (y_tr_df.shape, y_va_df.shape, y_all_df.shape))\n",
    "\n",
    "valid_indicators = np.hstack([\n",
    "    -1 * np.ones(y_tr_df.shape[0]), # -1 means never include this example in any test split\n",
    "    0  * np.ones(y_va_df.shape[0]), #  0 means include in the first test split (we count starting at 0 in python)\n",
    "    ])\n",
    "\n",
    "# Define custom splitter to use only the validation dataset for hyperparameter selection\n",
    "print(\"Splitter dimensions: %i\" % (valid_indicators.shape[0]))\n",
    "my_splitter = sklearn.model_selection.PredefinedSplit(valid_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = dict(activation=['relu', 'logistic', 'identity', 'tanh'], learning_rate_init=np.logspace(-5, 5, 100), learning_rate = ['constant','adaptive'], hidden_layer_sizes=[(20,),(50,),(100,),(200,),(500,)])\n",
    "\n",
    "fashion_pipes_norm = sklearn.pipeline.Pipeline([\n",
    "    ('rand_search', RandomizedSearchCV(sklearn.neural_network.MLPClassifier( solver='lbfgs', random_state = 0, shuffle=True, early_stopping = True), param_dist, scoring='balanced_accuracy', error_score='raise', random_state=0, return_train_score=True, n_iter=100, cv= my_splitter, n_jobs = -1, refit=False))\n",
    "])\n",
    "\n",
    "fashion_pipes_norm.fit(x_all_norm, y_all_df['class_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the same procedure with normalized input data - same results and same accuracy found in this case.display\n",
    "\n",
    "gridRes = fashion_pipes['rand_search'].cv_results_\n",
    "\n",
    "display(gridRes)\n",
    "bestIdxNorm = fashion_pipes['rand_search'].best_index_ # idx 12, 16, 44 all scored the same\n",
    "\n",
    "bestIdxs = [12,16,44]\n",
    "bestParams = dict()\n",
    "bestParams[0] = {k:v[bestIdxs[0]] for k,v in gridRes.items()}\n",
    "bestParams[1] = {k:v[bestIdxs[1]] for k,v in gridRes.items()}\n",
    "bestParams[2] = {k:v[bestIdxs[2]] for k,v in gridRes.items()}\n",
    "\n",
    "display(bestParams)\n",
    "# print(gridRes[\"mean_test_score\"])\n",
    "# display(gridRes[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMLP1bNorm = sklearn.neural_network.MLPClassifier( solver='lbfgs', random_state = 0, shuffle=True, early_stopping = True, learning_rate_init=1e-05,learning_rate ='constant',hidden_layer_sizes=(500,),activation='identity')\n",
    "bestMLP1bNorm.fit(x_tr_norm,y_tr_df[\"class_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc = sklearn.metrics.balanced_accuracy_score(y_tr_df['class_name'], bestMLP1bNorm.predict(x_tr_norm))\n",
    "va_acc = sklearn.metrics.balanced_accuracy_score(y_va_df['class_name'], bestMLP1bNorm.predict(x_va_norm))\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the validation accuracy seems to be lower on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_est_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting index for each class in training set:\n",
    "top_int = y_tr_df.index[y_tr_df['class_name']=='top']    #index 1\n",
    "trous_int = y_tr_df.index[y_tr_df['class_name']=='trouser']    #index 11\n",
    "dress_int =(y_tr_df.index[y_tr_df['class_name']=='dress']).tolist()\n",
    "pull_int =(y_tr_df.index[y_tr_df['class_name']=='pullover']).tolist()\n",
    "sneaker_int =(y_tr_df.index[y_tr_df['class_name']=='sneaker']).tolist()\n",
    "sandal_int =(y_tr_df.index[y_tr_df['class_name']=='sandal']).tolist()\n",
    "\n",
    "\n",
    "#sneakers + sandals + tops + trous + dress + pullovers in new dataset.\n",
    "train_x_2 = np.vstack((x_tr[sandal_int, :], x_tr[sneaker_int, :]))\n",
    "tops800= np.tile(x_tr[top_int, :], (800, 1)) #make 800 copies of tops\n",
    "train_x_2 = np.vstack((train_x_2, tops800)) # add 800 tops to sandals + sneakers\n",
    "assert(train_x_2.shape == (2400, 784))  #checking dimensions\n",
    "\n",
    "trous800 = np.tile(x_tr[trous_int, :], (800, 1))  #800 copies of trousers\n",
    "train_x_2 = np.vstack((train_x_2, trous800)) # add 800 trousers\n",
    "assert(train_x_2.shape == (3200, 784))  #checking dimensions\n",
    "\n",
    "dress800 = np.tile(x_tr[dress_int, :], (2, 1))   #doubled dresses 400 to 800\n",
    "assert(dress800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, dress800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4000, 784))  #checking dimensions\n",
    "\n",
    "pull800 = np.tile(x_tr[pull_int, :], (8, 1))\n",
    "assert(pull800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, pull800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4800, 784))\n",
    "\n",
    "#making train_y_2\n",
    "train_y_2 = np.tile(7, 800) \n",
    "train_y_2 = np.hstack((train_y_2, np.tile(5, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(0, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(1, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(3, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(2, 800)))\n",
    "\n",
    "assert(train_y_2.shape == (4800, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape: (4800, 784)\n",
      "Validation X shape: (600, 784)\n",
      "Combined X shape: (5400, 784)\n",
      "\n",
      "Training Y shape: (4800,)\n",
      "Validation Y shape: (600,)\n",
      "Combined Y shape: (5400,)\n",
      "\n",
      "Splitter dimensions: 5400\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation datasets\n",
    "x_dup_all = np.vstack((train_x_2, x_va))\n",
    "y_dup_all = np.hstack((train_y_2, y_va_df['class_uid']))\n",
    "\n",
    "print(\"Training X shape: %s\\nValidation X shape: %s\\nCombined X shape: %s\\n\" % (train_x_2.shape, x_va.shape, x_dup_all.shape))\n",
    "print(\"Training Y shape: %s\\nValidation Y shape: %s\\nCombined Y shape: %s\\n\" % (train_y_2.shape, y_va_df['class_uid'].shape, y_dup_all.shape))\n",
    "\n",
    "valid_dup_indicators = np.hstack([\n",
    "    -1 * np.ones(train_y_2.shape[0]), #-1 = exclude this example in test split\n",
    "    0 * np.ones(y_va_df.shape[0]), #0 = include in the first test split\n",
    "    ])\n",
    "\n",
    "# Define custom splitter to use only the validation dataset for hyperparameter selection\n",
    "print(\"Splitter dimensions: %i\" % (valid_dup_indicators.shape[0]))\n",
    "my_dup_splitter = sklearn.model_selection.PredefinedSplit(valid_dup_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = dict(activation=['relu', 'logistic', 'identity', 'tanh'], learning_rate_init=np.logspace(-5, 5, 100), learning_rate = ['constant','adaptive'], hidden_layer_sizes=[(20,),(50,),(100,),(200,),(500,)], random_state = [0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With just 4 iterations in Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model4 = pickle.load(open('1D_4_iter_initial_search.sav', 'rb'))\n",
    "result4 = loaded_model4.score(x_dup_all, y_dup_all)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the below parameters with n_iter = 4 had same test score 0.445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'random_state': 3,\n",
    " 'learning_rate_init': 4.037017258596558e-05,\n",
    " 'learning_rate': 'constant',\n",
    " 'hidden_layer_sizes': (100,),\n",
    " 'activation': 'logistic'}\n",
    "\n",
    "{'random_state': 6,\n",
    "   'learning_rate_init': 4.037017258596558e-05,\n",
    "   'learning_rate': 'constant',\n",
    "   'hidden_layer_sizes': (500,),\n",
    "   'activation': 'identity'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With just 16 iterations in Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist16 = dict(learning_rate_init=np.logspace(-5, 5, 100), learning_rate = ['constant','adaptive'], hidden_layer_sizes=[(20,),(50,),(100,),(200,),(500,)], random_state = [0, 1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model16 = pickle.load(open('1D_16_iter_initial_search.sav', 'rb'))\n",
    "result16 = loaded_model16.score(x_dup_all, y_dup_all)\n",
    "print(result16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'random_state': 5,\n",
    "   'learning_rate_init': 954.5484566618328,\n",
    "   'learning_rate': 'adaptive',\n",
    "   'hidden_layer_sizes': (20,),\n",
    "   'activation': 'tanh'},\n",
    "   \n",
    "{'random_state': 4,\n",
    "   'learning_rate_init': 0.00041320124001153346,\n",
    "   'learning_rate': 'constant',\n",
    "   'hidden_layer_sizes': (20,),\n",
    "   'activation': 'identity'},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 iterations Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model64 = pickle.load(open('1D_64_iter_initial_search.sav', 'rb'))\n",
    "result64 = loaded_model64.score(x_dup_all, y_dup_all)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 12.98047519,  57.53519821,  61.01048613, 101.27840877,\n",
       "        154.14830899,  14.68329239,  35.75348592,  54.83108926,\n",
       "        123.53866339,  14.62633729,   4.29979658,   4.4848721 ,\n",
       "         19.99948883,  20.77584076,  61.2773695 ,  96.31079388]),\n",
       " 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'mean_score_time': array([0.02224731, 0.00521946, 0.0167346 , 0.01674795, 0.02640319,\n",
       "        0.01655626, 0.01300073, 0.01639867, 0.01231408, 0.01347518,\n",
       "        0.        , 0.        , 0.01666713, 0.00688744, 0.01781321,\n",
       "        0.03361917]),\n",
       " 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'param_random_state': masked_array(data=[2, 5, 3, 6, 5, 5, 6, 6, 2, 5, 4, 4, 2, 5, 3, 0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[12328.467394420633, 0.1747528400007683,\n",
       "                    4.037017258596558e-05, 4.037017258596558e-05,\n",
       "                    298.364724028334, 0.027185882427329403,\n",
       "                    0.0005214008287999684, 58.57020818056661,\n",
       "                    36.783797718286344, 954.5484566618328,\n",
       "                    0.00041320124001153346, 0.021544346900318846,\n",
       "                    2.5353644939701114e-05, 14.508287784959402,\n",
       "                    23.10129700083158, 0.04328761281083057],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=['adaptive', 'adaptive', 'constant', 'constant',\n",
       "                    'constant', 'constant', 'adaptive', 'constant',\n",
       "                    'adaptive', 'adaptive', 'constant', 'adaptive',\n",
       "                    'constant', 'constant', 'adaptive', 'constant'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(50,), (100,), (100,), (500,), (500,), (20,), (50,),\n",
       "                    (100,), (500,), (20,), (20,), (20,), (50,), (50,),\n",
       "                    (500,), (500,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_activation': masked_array(data=['relu', 'logistic', 'logistic', 'identity', 'logistic',\n",
       "                    'tanh', 'logistic', 'tanh', 'identity', 'tanh',\n",
       "                    'identity', 'identity', 'identity', 'identity', 'relu',\n",
       "                    'identity'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'random_state': 2,\n",
       "   'learning_rate_init': 12328.467394420633,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'activation': 'relu'},\n",
       "  {'random_state': 5,\n",
       "   'learning_rate_init': 0.1747528400007683,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (100,),\n",
       "   'activation': 'logistic'},\n",
       "  {'random_state': 3,\n",
       "   'learning_rate_init': 4.037017258596558e-05,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (100,),\n",
       "   'activation': 'logistic'},\n",
       "  {'random_state': 6,\n",
       "   'learning_rate_init': 4.037017258596558e-05,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (500,),\n",
       "   'activation': 'identity'},\n",
       "  {'random_state': 5,\n",
       "   'learning_rate_init': 298.364724028334,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (500,),\n",
       "   'activation': 'logistic'},\n",
       "  {'random_state': 5,\n",
       "   'learning_rate_init': 0.027185882427329403,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (20,),\n",
       "   'activation': 'tanh'},\n",
       "  {'random_state': 6,\n",
       "   'learning_rate_init': 0.0005214008287999684,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'activation': 'logistic'},\n",
       "  {'random_state': 6,\n",
       "   'learning_rate_init': 58.57020818056661,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (100,),\n",
       "   'activation': 'tanh'},\n",
       "  {'random_state': 2,\n",
       "   'learning_rate_init': 36.783797718286344,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (500,),\n",
       "   'activation': 'identity'},\n",
       "  {'random_state': 5,\n",
       "   'learning_rate_init': 954.5484566618328,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (20,),\n",
       "   'activation': 'tanh'},\n",
       "  {'random_state': 4,\n",
       "   'learning_rate_init': 0.00041320124001153346,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (20,),\n",
       "   'activation': 'identity'},\n",
       "  {'random_state': 4,\n",
       "   'learning_rate_init': 0.021544346900318846,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (20,),\n",
       "   'activation': 'identity'},\n",
       "  {'random_state': 2,\n",
       "   'learning_rate_init': 2.5353644939701114e-05,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'activation': 'identity'},\n",
       "  {'random_state': 5,\n",
       "   'learning_rate_init': 14.508287784959402,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'activation': 'identity'},\n",
       "  {'random_state': 3,\n",
       "   'learning_rate_init': 23.10129700083158,\n",
       "   'learning_rate': 'adaptive',\n",
       "   'hidden_layer_sizes': (500,),\n",
       "   'activation': 'relu'},\n",
       "  {'random_state': 0,\n",
       "   'learning_rate_init': 0.04328761281083057,\n",
       "   'learning_rate': 'constant',\n",
       "   'hidden_layer_sizes': (500,),\n",
       "   'activation': 'identity'}],\n",
       " 'split0_test_score': array([0.34833333, 0.37833333, 0.445     , 0.445     , 0.35333333,\n",
       "        0.37333333, 0.48      , 0.36833333, 0.34333333, 0.37333333,\n",
       "        0.48666667, 0.48666667, 0.35166667, 0.47166667, 0.335     ,\n",
       "        0.35666667]),\n",
       " 'mean_test_score': array([0.34833333, 0.37833333, 0.445     , 0.445     , 0.35333333,\n",
       "        0.37333333, 0.48      , 0.36833333, 0.34333333, 0.37333333,\n",
       "        0.48666667, 0.48666667, 0.35166667, 0.47166667, 0.335     ,\n",
       "        0.35666667]),\n",
       " 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'rank_test_score': array([14,  7,  5,  5, 12,  8,  3, 10, 15,  8,  1,  1, 13,  4, 16, 11]),\n",
       " 'split0_train_score': array([1.        , 0.97895833, 0.975     , 1.        , 0.99666667,\n",
       "        0.96041667, 0.97041667, 0.96916667, 1.        , 0.96041667,\n",
       "        0.77645833, 0.77645833, 1.        , 0.795     , 1.        ,\n",
       "        1.        ]),\n",
       " 'mean_train_score': array([1.        , 0.97895833, 0.975     , 1.        , 0.99666667,\n",
       "        0.96041667, 0.97041667, 0.96916667, 1.        , 0.96041667,\n",
       "        0.77645833, 0.77645833, 1.        , 0.795     , 1.        ,\n",
       "        1.        ]),\n",
       " 'std_train_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_fashion_pipes64['rand_search'].cv_results_ \n",
    "#Best score was 0.486. Same as when we did 16 iters. Not necessary to do 64\n",
    "{'random_state': 4,\n",
    "   'learning_rate_init': 0.00041320124001153346,\n",
    "   'learning_rate': 'constant',\n",
    "   'hidden_layer_sizes': (20,),\n",
    "   'activation': 'identity'}\n",
    "{'random_state': 4,\n",
    "   'learning_rate_init': 0.021544346900318846,\n",
    "   'learning_rate': 'adaptive',\n",
    "   'hidden_layer_sizes': (20,),\n",
    "   'activation': 'identity'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now narrowing - 16 iters 20 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag1 = 'go'\n",
    "\n",
    "#Gave activation function logistic and hidden layer 20. Gave 200 iterations.\n",
    "#   ALL came out to have scores 0.528333... Trying random seeds 0-4\n",
    "\n",
    "if flag1 != 'stop':\n",
    "    param_dist2 = dict(learning_rate_init=np.logspace(-5, 5, 100), learning_rate = ['constant','adaptive'], random_state=[0, 1, 2, 3, 4])\n",
    "\n",
    "    dup_fashion_pipes2 = sklearn.pipeline.Pipeline([\n",
    "        ('rand_search', RandomizedSearchCV(sklearn.neural_network.MLPClassifier (activation='identity', solver='lbfgs', shuffle=True, early_stopping = True, hidden_layer_sizes=(20,)), param_dist2, scoring='balanced_accuracy', error_score='raise', return_train_score=True, n_iter=16, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "    ])\n",
    "\n",
    "    dup_fashion_pipes2.fit(x_dup_all, y_dup_all)\n",
    "    # filename = '1D_second_RanSearch.sav'\n",
    "    # pickle.dump(dup_fashion_pipes, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48666667 0.37333333 0.54166667 0.37333333 0.30333333 0.30333333\n",
      " 0.37333333 0.54166667 0.48666667 0.37333333 0.48666667 0.48666667\n",
      " 0.48333333 0.30333333 0.48333333 0.30333333]\n"
     ]
    }
   ],
   "source": [
    "scores2 = dup_fashion_pipes2['rand_search'].cv_results_['mean_test_score']\n",
    "print(scores2)\n",
    "#best scores on index 10 and 13 with 0.54!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 1,\n",
       " 'learning_rate_init': 0.5590810182512223,\n",
       " 'learning_rate': 'adaptive'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_fashion_pipes2['rand_search'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'random_state': 1,\n",
    "   'learning_rate_init': 0.5590810182512223,\n",
    "   'learning_rate': 'adaptive'},\n",
    "{'random_state': 1,\n",
    "   'learning_rate_init': 24770.76355991714,\n",
    "   'learning_rate': 'constant'},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;rand_search&#x27;,\n",
       "                 RandomizedSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "                                    estimator=MLPClassifier(random_state=0,\n",
       "                                                            solver=&#x27;lbfgs&#x27;),\n",
       "                                    n_iter=5,\n",
       "                                    param_distributions={&#x27;activation&#x27;: [&#x27;relu&#x27;,\n",
       "                                                                        &#x27;logistic&#x27;,\n",
       "                                                                        &#x27;identity&#x27;,\n",
       "                                                                        &#x27;tanh&#x27;],\n",
       "                                                         &#x27;batch_size&#x27;: [10, 25,\n",
       "                                                                        50, 100,\n",
       "                                                                        200,\n",
       "                                                                        500],\n",
       "                                                         &#x27;learning_rate_init&#x27;: array([1.00000000e-05, 1.26185688e-05, 1.59228279e-05, 2.00923300e-05,\n",
       "       2.53536449e-05,...\n",
       "       1.20450354e+03, 1.51991108e+03, 1.91791026e+03, 2.42012826e+03,\n",
       "       3.05385551e+03, 3.85352859e+03, 4.86260158e+03, 6.13590727e+03,\n",
       "       7.74263683e+03, 9.77009957e+03, 1.23284674e+04, 1.55567614e+04,\n",
       "       1.96304065e+04, 2.47707636e+04, 3.12571585e+04, 3.94420606e+04,\n",
       "       4.97702356e+04, 6.28029144e+04, 7.92482898e+04, 1.00000000e+05])},\n",
       "                                    random_state=0, return_train_score=True,\n",
       "                                    scoring=&#x27;balanced_accuracy&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rand_search&#x27;,\n",
       "                 RandomizedSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "                                    estimator=MLPClassifier(random_state=0,\n",
       "                                                            solver=&#x27;lbfgs&#x27;),\n",
       "                                    n_iter=5,\n",
       "                                    param_distributions={&#x27;activation&#x27;: [&#x27;relu&#x27;,\n",
       "                                                                        &#x27;logistic&#x27;,\n",
       "                                                                        &#x27;identity&#x27;,\n",
       "                                                                        &#x27;tanh&#x27;],\n",
       "                                                         &#x27;batch_size&#x27;: [10, 25,\n",
       "                                                                        50, 100,\n",
       "                                                                        200,\n",
       "                                                                        500],\n",
       "                                                         &#x27;learning_rate_init&#x27;: array([1.00000000e-05, 1.26185688e-05, 1.59228279e-05, 2.00923300e-05,\n",
       "       2.53536449e-05,...\n",
       "       1.20450354e+03, 1.51991108e+03, 1.91791026e+03, 2.42012826e+03,\n",
       "       3.05385551e+03, 3.85352859e+03, 4.86260158e+03, 6.13590727e+03,\n",
       "       7.74263683e+03, 9.77009957e+03, 1.23284674e+04, 1.55567614e+04,\n",
       "       1.96304065e+04, 2.47707636e+04, 3.12571585e+04, 3.94420606e+04,\n",
       "       4.97702356e+04, 6.28029144e+04, 7.92482898e+04, 1.00000000e+05])},\n",
       "                                    random_state=0, return_train_score=True,\n",
       "                                    scoring=&#x27;balanced_accuracy&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rand_search: RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "                   estimator=MLPClassifier(random_state=0, solver=&#x27;lbfgs&#x27;),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;logistic&#x27;,\n",
       "                                                       &#x27;identity&#x27;, &#x27;tanh&#x27;],\n",
       "                                        &#x27;batch_size&#x27;: [10, 25, 50, 100, 200,\n",
       "                                                       500],\n",
       "                                        &#x27;learning_rate_init&#x27;: array([1.00000000e-05, 1.26185688e-05, 1.59228279e-05, 2.00923300e-05,\n",
       "       2.53536449e-05, 3.19926714e-05, 4.03701726e-05, 5...\n",
       "       1.20450354e+03, 1.51991108e+03, 1.91791026e+03, 2.42012826e+03,\n",
       "       3.05385551e+03, 3.85352859e+03, 4.86260158e+03, 6.13590727e+03,\n",
       "       7.74263683e+03, 9.77009957e+03, 1.23284674e+04, 1.55567614e+04,\n",
       "       1.96304065e+04, 2.47707636e+04, 3.12571585e+04, 3.94420606e+04,\n",
       "       4.97702356e+04, 6.28029144e+04, 7.92482898e+04, 1.00000000e+05])},\n",
       "                   random_state=0, return_train_score=True,\n",
       "                   scoring=&#x27;balanced_accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(random_state=0, solver=&#x27;lbfgs&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(random_state=0, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('rand_search',\n",
       "                 RandomizedSearchCV(cv=5, error_score='raise',\n",
       "                                    estimator=MLPClassifier(random_state=0,\n",
       "                                                            solver='lbfgs'),\n",
       "                                    n_iter=5,\n",
       "                                    param_distributions={'activation': ['relu',\n",
       "                                                                        'logistic',\n",
       "                                                                        'identity',\n",
       "                                                                        'tanh'],\n",
       "                                                         'batch_size': [10, 25,\n",
       "                                                                        50, 100,\n",
       "                                                                        200,\n",
       "                                                                        500],\n",
       "                                                         'learning_rate_init': array([1.00000000e-05, 1.26185688e-05, 1.59228279e-05, 2.00923300e-05,\n",
       "       2.53536449e-05,...\n",
       "       1.20450354e+03, 1.51991108e+03, 1.91791026e+03, 2.42012826e+03,\n",
       "       3.05385551e+03, 3.85352859e+03, 4.86260158e+03, 6.13590727e+03,\n",
       "       7.74263683e+03, 9.77009957e+03, 1.23284674e+04, 1.55567614e+04,\n",
       "       1.96304065e+04, 2.47707636e+04, 3.12571585e+04, 3.94420606e+04,\n",
       "       4.97702356e+04, 6.28029144e+04, 7.92482898e+04, 1.00000000e+05])},\n",
       "                                    random_state=0, return_train_score=True,\n",
       "                                    scoring='balanced_accuracy'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "param_dist3 = dict(learning_rate_init=np.logspace(-5, 0, 2), learning_rate = ['constant','adaptive'], random_state=[0, 1, 2, 3, 4])\n",
    "\n",
    "dup_fashion_pipes3 = sklearn.pipeline.Pipeline([\n",
    "    ('grid_search', sklearn.GridSearchCV(sklearn.neural_network.MLPClassifier (activation='identity', solver='lbfgs', shuffle=True, early_stopping = True, hidden_layer_sizes=(20,)), param_dist2, scoring='balanced_accuracy', error_score='raise', return_train_score=True, n_iter=16, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "])\n",
    "\n",
    "dup_fashion_pipes3.fit(x_dup_all, y_dup_all)\n",
    "# filename = '1D_second_RanSearch.sav'\n",
    "# pickle.dump(dup_fashion_pipes, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
