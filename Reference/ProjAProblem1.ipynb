{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.feature_selection\n",
    "import sklearn.pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our filess\n",
    "from load_train_data import load_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list, review_list, rating_list = load_data('x_train.csv', 'y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression classifier\n",
    "logistic = sklm.LogisticRegression(solver='liblinear')\n",
    "# Hyperparameters distributions - regularization strength C (10E-4 to 10E6, originally 10E-9 to 10E6),\n",
    "# penalty (L1, L2), max iterations (80 to 100, originally 1 to 50000)\n",
    "distributions = dict(C=np.logspace(-2,2,200), penalty = ['l2', 'l1'], max_iter = \n",
    "                     np.logspace(np.log10(80),np.log10(200),50, dtype=int)) \n",
    "# Number of folds for cross validation\n",
    "numFolds = 5\n",
    "\n",
    "#Pipeline starts!\n",
    "my_bow_classifier_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=2, max_df=1.0, ngram_range=(1,1))),\n",
    "    ('cross validation', skms.RandomizedSearchCV(logistic, distributions, n_iter=200, cv=numFolds, verbose=0, random_state=0, error_score='raise',\n",
    "                                                  scoring='roc_auc', return_train_score=True))\n",
    "])\n",
    "\n",
    "my_bow_classifier_pipeline.fit(review_list, rating_list)\n",
    "# my_bow_classifier_pipeline.predict(review_list)\n",
    "# my_bow_classifier_pipeline.score(review_list, rating_list)\n",
    "probs = my_bow_classifier_pipeline.predict_proba(review_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get useful data out from the random search\n",
    "optParams = my_bow_classifier_pipeline['cross validation'].best_params_\n",
    "print('Best parameters: ', optParams)\n",
    "# This final logistic regression classifier is the best estimator from the random search, \n",
    "# and used the chosen hyperparameters (optParams) and the entire training set.\n",
    "finalEstimator = my_bow_classifier_pipeline.named_steps['cross validation'].best_estimator_\n",
    "\n",
    "\n",
    "# my_bow_classifier_pipeline.named_steps['cross validation'].cv_results_\n",
    "\n",
    "# cvKeys = list(cvRes.keys())\n",
    "# print(type(cvKeys))\n",
    "# print(cvRes['rank_test_score'])\n",
    "\n",
    "# gsearch_results_df = pd.DataFrame(cvRes).copy()\n",
    "# param_keys = cvKeys\n",
    "\n",
    "# # Rearrange row order so it is easy to skim\n",
    "# gsearch_results_df.sort_values(param_keys, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = my_bow_classifier_pipeline['cross validation'].best_estimator_.coef_\n",
    "\n",
    "#getting CountVectorizer dictionary\n",
    "dictionary = my_bow_classifier_pipeline['my_bow_feature_extractor'].vocabulary_\n",
    "\n",
    "print(my_bow_classifier_pipeline['cross validation'].best_params_)\n",
    "\n",
    "acc = roc_auc_score(rating_list, probs[:,1])\n",
    "print(\"AUROC on full Training set: %.3f\" % acc)\n",
    "\n",
    "ResultsCV = my_bow_classifier_pipeline['cross validation'].cv_results_\n",
    "optIter = my_bow_classifier_pipeline['cross validation'].best_index_\n",
    "Res_Train = ResultsCV['mean_train_score'][optIter]\n",
    "Res_Val = ResultsCV['mean_test_score'][optIter]\n",
    "\n",
    "print(\"AUROC on Training folds: %.4f\" % Res_Train)\n",
    "print(\"AUROC on Heldout folds: %.4f\" % Res_Val)\n",
    "print('Length of vocabulary is %i' % len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining optimum df values\n",
    "min_df = 1, max_df = 0.08\n",
    "```python\n",
    "{'penalty': 'l2', 'max_iter': 79, 'C': 1.1226677735108135}\n",
    "AUROC on full Training set: 0.994\n",
    "AUROC on Training folds: 0.9952\n",
    "AUROC on Heldout folds: 0.8606\n",
    "```\n",
    "\n",
    "min_df = 1, max_df = 1.0\n",
    "```python\n",
    "{'penalty': 'l2', 'max_iter': 94, 'C': 1.7834308769319094}\n",
    "AUROC on full Training set: 0.997\n",
    "AUROC on Training folds: 0.9981\n",
    "AUROC on Heldout folds: 0.8727\n",
    "```\n",
    "\n",
    "min_df = 2, max_df = 1.0\n",
    "```python\n",
    "{'penalty': 'l2', 'max_iter': 98, 'C': 1.7027691722258995}\n",
    "AUROC on full Training set: 0.993\n",
    "AUROC on Training folds: 0.9941\n",
    "AUROC on Heldout folds: 0.8734\n",
    "```\n",
    "\n",
    "min_df = 3, max_df = 1.0\n",
    "```python\n",
    "{'penalty': 'l2', 'max_iter': 120, 'C': 0.890735463861044}\n",
    "AUROC on full Training set: 0.979\n",
    "AUROC on Training folds: 0.9822\n",
    "AUROC on Heldout folds: 0.8676\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = my_bow_classifier_pipeline.predict(review_list)\n",
    "yhatProbs = my_bow_classifier_pipeline.predict_proba(review_list)\n",
    "print(yhat)\n",
    "\n",
    "for revIdx in np.arange(len(yhat)):\n",
    "    if yhat[revIdx] != rating_list[revIdx]:\n",
    "        print('True %s|%s Pred| %1.3f| %s' % (rating_list[revIdx], yhat[revIdx], yhatProbs[revIdx][1], review_list[revIdx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresh = roc_curve(rating_list, probs[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr, label = 'ROC curve (area = %0.3f)' % acc)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "x_te_data = 'x_test.csv'\n",
    "data_dir = 'data_reviews'\n",
    "x_te_df = pd.read_csv(os.path.join(data_dir, x_te_data))\n",
    "te_website_list = x_te_df['website_name'].values.tolist()\n",
    "te_text_list = x_te_df['text'].values.tolist()\n",
    "\n",
    "probs = my_bow_classifier_pipeline.predict_proba(te_text_list)[:, 1]\n",
    "print(probs.shape)\n",
    "\n",
    "# Save predictions to file (need to rename for upload)\n",
    "np.savetxt('q1.txt', probs, fmt='%s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate impact of each hyperparameter, using \"best\" parameters for all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate C!\n",
    "\n",
    "# Create a logistic regression classifier\n",
    "logisticEvalHype = sklm.LogisticRegression(solver='liblinear',penalty=optParams['penalty'], max_iter=optParams['max_iter'])\n",
    "# Hyperparameters distributions - regularization strength C\n",
    "numFolds = 5\n",
    "CDist = np.logspace(-9,6,40)\n",
    "distEvalHype = dict(C = CDist) #[10**(-4), optParams['C'], 10**6])\n",
    "\n",
    "#Pipeline starts!\n",
    "evalHypeParamCPipe = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=1, max_df=0.08, ngram_range=(1,1))),\n",
    "    ('cross validation', skms.GridSearchCV(logisticEvalHype, distEvalHype, cv=numFolds, verbose=0, \n",
    "                                                error_score='raise', scoring='roc_auc', return_train_score=True))\n",
    "])\n",
    "\n",
    "# (shuffleReviews, shuffledRatings) = shuffle(review_list, rating_list, random_state=0)\n",
    "# evalHypeParamCPipe.fit(shuffleReviews, shuffledRatings)\n",
    "evalHypeParamCPipe.fit(review_list, rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate max_iter!\n",
    "\n",
    "# Create a logistic regression classifier\n",
    "logisticEvalHype = sklm.LogisticRegression(solver='liblinear',penalty=optParams['penalty'], C=optParams['C'])\n",
    "# Hyperparameters distributions - regularization strength C\n",
    "maxIterDist = np.logspace(np.log10(1),np.log10(50000),50, dtype=int)\n",
    "distEvalHype = dict(max_iter = maxIterDist) \n",
    "\n",
    "#Pipeline starts!\n",
    "evalHypeParamIterPipe = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=1, max_df=0.08, ngram_range=(1,1))),\n",
    "    ('cross validation', skms.GridSearchCV(logisticEvalHype, distEvalHype, cv=5, verbose=0, \n",
    "                                                error_score='raise', scoring='roc_auc', return_train_score=True))\n",
    "])\n",
    "\n",
    "\n",
    "# Shuffle data\n",
    "# (shuffleReviews, shuffledRatings) = shuffle(review_list, rating_list, random_state=0)\n",
    "# evalHypeParamIterPipe.fit(shuffleReviews, shuffledRatings)\n",
    "evalHypeParamIterPipe.fit(review_list, rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract C data for plotting\n",
    "\n",
    "CVars = evalHypeParamCPipe['cross validation'].cv_results_\n",
    "\n",
    "C_Train = CVars['mean_train_score']\n",
    "C_Val = CVars['mean_test_score']\n",
    "C_TrainFolds = np.vstack((CVars['split0_train_score'], CVars['split1_train_score'], CVars['split2_train_score'], CVars['split3_train_score'], CVars['split4_train_score']))\n",
    "C_ValFolds = np.vstack((CVars['split0_test_score'], CVars['split1_test_score'], CVars['split2_test_score'], CVars['split3_test_score'], CVars['split4_test_score']))\n",
    "CDistFolds = np.tile(CDist, (numFolds,1))\n",
    "print(CDistFolds.shape)\n",
    "\n",
    "# Extract max_iter data for plotting\n",
    "IterVars = evalHypeParamIterPipe['cross validation'].cv_results_\n",
    "\n",
    "Iter_Train = IterVars['mean_train_score']\n",
    "Iter_Val = IterVars['mean_test_score']\n",
    "Iter_TrainFolds = np.vstack((IterVars['split0_train_score'], IterVars['split1_train_score'], IterVars['split2_train_score'], IterVars['split3_train_score'], IterVars['split4_train_score']))\n",
    "Iter_ValFolds = np.vstack((IterVars['split0_test_score'], IterVars['split1_test_score'], IterVars['split2_test_score'], IterVars['split3_test_score'], IterVars['split4_test_score']))\n",
    "IterDistFolds = np.tile(maxIterDist, (numFolds,1))\n",
    "\n",
    "# Plot accuracy vs C and max_iter hyperparameters\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(10,8))\n",
    "ax1.scatter(CDistFolds, C_TrainFolds, label='Training AUROC from Each Fold',marker='.')\n",
    "ax1.scatter(CDistFolds, C_ValFolds, label='Heldout AUROC from Each Fold',marker='.')\n",
    "ax1.plot(CDist, C_Train, label='Mean Training AUROC')\n",
    "ax1.plot(CDist, C_Val, label='Mean Heldout AUROC')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('C')\n",
    "ax1.set_ylabel('AUROC')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(which='both')\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "ax2.scatter(IterDistFolds, Iter_TrainFolds, label='Training AUROC from Each Fold',marker='.')\n",
    "ax2.scatter(IterDistFolds, Iter_ValFolds, label='Heldout AUROC from Each Fold',marker='.')\n",
    "ax2.plot(maxIterDist, Iter_Train, label='Mean Training AUROC')\n",
    "ax2.plot(maxIterDist, Iter_Val, label='Mean Heldout AUROC')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1, 0.8))\n",
    "ax2.set_xlabel('Max Iterations')\n",
    "ax2.set_ylabel('AUROC')\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(which='major')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
