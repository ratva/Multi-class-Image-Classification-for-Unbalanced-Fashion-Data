{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "# Import our custom functions\n",
    "from load_data import load_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr_df = load_data('x_train.csv', 'y_train.csv')\n",
    "x_va, y_va_df = load_data('x_valid.csv', 'y_valid.csv')\n",
    "x_te = load_data('x_test.csv', 'y_valid.csv')[0]\n",
    "\n",
    "\n",
    "for label, arr in [('train', x_tr), ('valid', x_va)]:\n",
    "    print(\"Contents of %s_x.csv: arr of shape %s\" % (\n",
    "        label, str(arr.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting index for each class in training set:\n",
    "top_int = y_tr_df.index[y_tr_df['class_name']=='top']    #index 1\n",
    "trous_int = y_tr_df.index[y_tr_df['class_name']=='trouser']    #index 11\n",
    "dress_int =(y_tr_df.index[y_tr_df['class_name']=='dress']).tolist()\n",
    "pull_int =(y_tr_df.index[y_tr_df['class_name']=='pullover']).tolist()\n",
    "sneaker_int =(y_tr_df.index[y_tr_df['class_name']=='sneaker']).tolist()\n",
    "sandal_int =(y_tr_df.index[y_tr_df['class_name']=='sandal']).tolist()\n",
    "\n",
    "\n",
    "#sneakers + sandals + tops + trous + dress + pullovers in new dataset.\n",
    "train_x_2 = np.vstack((x_tr[sandal_int, :], x_tr[sneaker_int, :]))\n",
    "tops800= np.tile(x_tr[top_int, :], (800, 1)) #make 800 copies of tops\n",
    "train_x_2 = np.vstack((train_x_2, tops800)) # add 800 tops to sandals + sneakers\n",
    "assert(train_x_2.shape == (2400, 784))  #checking dimensions\n",
    "\n",
    "trous800 = np.tile(x_tr[trous_int, :], (800, 1))  #800 copies of trousers\n",
    "train_x_2 = np.vstack((train_x_2, trous800)) # add 800 trousers\n",
    "assert(train_x_2.shape == (3200, 784))  #checking dimensions\n",
    "\n",
    "dress800 = np.tile(x_tr[dress_int, :], (2, 1))   #doubled dresses 400 to 800\n",
    "assert(dress800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, dress800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4000, 784))  #checking dimensions\n",
    "\n",
    "pull800 = np.tile(x_tr[pull_int, :], (8, 1))\n",
    "assert(pull800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, pull800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4800, 784))\n",
    "\n",
    "#making train_y_2\n",
    "train_y_2 = np.tile(7, 800) \n",
    "train_y_2 = np.hstack((train_y_2, np.tile(5, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(0, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(1, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(3, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(2, 800)))\n",
    "\n",
    "assert(train_y_2.shape == (4800, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and validation datasets\n",
    "x_dup_all = np.vstack((train_x_2, x_va))\n",
    "y_dup_all = np.hstack((train_y_2, y_va_df['class_uid']))\n",
    "\n",
    "print(\"Training X shape: %s\\nValidation X shape: %s\\nCombined X shape: %s\\n\" % (train_x_2.shape, x_va.shape, x_dup_all.shape))\n",
    "print(\"Training Y shape: %s\\nValidation Y shape: %s\\nCombined Y shape: %s\\n\" % (train_y_2.shape, y_va_df['class_uid'].shape, y_dup_all.shape))\n",
    "\n",
    "valid_dup_indicators = np.hstack([\n",
    "    -1 * np.ones(train_y_2.shape[0]), #-1 = exclude this example in test split\n",
    "    0 * np.ones(y_va_df.shape[0]), #0 = include in the first test split\n",
    "    ])\n",
    "\n",
    "# Define custom splitter to use only the validation dataset for hyperparameter selection\n",
    "print(\"Splitter dimensions: %i\" % (valid_dup_indicators.shape[0]))\n",
    "my_dup_splitter = sklearn.model_selection.PredefinedSplit(valid_dup_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag1 = 'stop'\n",
    "\n",
    "if flag1 != 'stop':\n",
    "    #batch size only for non-lbfgs. \n",
    "    #Learning Rate = const, adaptive, etc... ONLY for sgd\n",
    "    #early_stopping only for sgd/adam.\n",
    "    ran_params = dict(alpha = np.logspace(-5, 5, 100), learning_rate_init=np.logspace(-5, 5, 100), max_iter = [50, 100, 200, 300], hidden_layer_sizes=[(10,), (20,),(50,),(100,),(200,),(500,)])\n",
    "\n",
    "    ran_dup_fashion_pipes = sklearn.pipeline.Pipeline([\n",
    "        ('rand_search', RandomizedSearchCV(sklearn.neural_network.MLPClassifier (activation='identity', solver='lbfgs', shuffle=True, random_state=0), ran_params, scoring='balanced_accuracy', error_score='raise', return_train_score=True, n_iter=100, cv= my_dup_splitter, n_jobs = -1, refit= False, random_state=0))\n",
    "    ])\n",
    "\n",
    "    ran_dup_fashion_pipes.fit(x_dup_all, y_dup_all)\n",
    "    filename = '1D_initial_RanSearch.sav'\n",
    "    pickle.dump(ran_dup_fashion_pipes, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index= np.argsort(ran_dup_fashion_pipes['rand_search'].cv_results_['rank_test_score'])\n",
    "for i in best_index:\n",
    "    score =ran_dup_fashion_pipes['rand_search'].cv_results_['mean_test_score'][i]\n",
    "    params = ran_dup_fashion_pipes['rand_search'].cv_results_['params'][i]\n",
    "    print(\"mean_test_score: %f\" % (score), \"params: %s\" %params)\n",
    "# the BEST score 0.565 mean test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag1 = 'go'\n",
    "\n",
    "if flag1 != 'stop':\n",
    "    ran_params2 = dict(alpha = np.logspace(-5, 3, 100), learning_rate_init=np.logspace(-5, 5, 100), max_iter = [5, 10, 25, 26, 40, 290, 300, 310, 311, 312])\n",
    "\n",
    "    ran_dup_fashion_pipes2 = sklearn.pipeline.Pipeline([\n",
    "        ('rand_search', RandomizedSearchCV(sklearn.neural_network.MLPClassifier (activation='identity', solver='lbfgs', shuffle=True, random_state=0, hidden_layer_sizes= (10,)), ran_params2, scoring='balanced_accuracy', error_score='raise', return_train_score=True, n_iter=100, cv= my_dup_splitter, n_jobs = -1, refit= False, random_state=0))\n",
    "    ])\n",
    "\n",
    "    ran_dup_fashion_pipes2.fit(x_dup_all, y_dup_all)\n",
    "    filename = '1D_2nd_RanSearch.sav'\n",
    "    pickle.dump(ran_dup_fashion_pipes2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index= np.argsort(ran_dup_fashion_pipes2['rand_search'].cv_results_['rank_test_score'])\n",
    "for i in best_index:\n",
    "    score =ran_dup_fashion_pipes2['rand_search'].cv_results_['mean_test_score'][i]\n",
    "    params = ran_dup_fashion_pipes2['rand_search'].cv_results_['params'][i]\n",
    "    print(\"mean_test_score: %f\" % (score), \"params: %s\" %params)\n",
    "# the BEST score 0.565 mean test\n",
    "#mean_test_score: 0.621667 params: {'max_iter': 40, 'learning_rate_init': 2.0092330025650458e-05, 'hidden_layer_sizes': (10,), 'alpha': 62802.914418342465}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distEvalHype = dict(learning_rate_init = np.logspace(-2, 2, 50), alpha = np.logspace(-5, 5, 50), random_state = [0, 1, 2], max_iter = np.linspace(1, 400, 50, dtype=int))\n",
    "\n",
    "grid_dup_fashion_pipes = sklearn.pipeline.Pipeline([\n",
    "('grid_search', GridSearchCV(sklearn.neural_network.MLPClassifier\n",
    "                             (activation='identity', solver='lbfgs', learning_rate= 'constant', shuffle=True, early_stopping = True, random_state=0, hidden_layer_sizes = (10, )), distEvalHype,\n",
    "                              scoring='balanced_accuracy', error_score='raise',\n",
    "                              return_train_score=True, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "])\n",
    "grid_dup_fashion_pipes.fit(x_dup_all, y_dup_all)\n",
    "\n",
    "\n",
    "# filename = '1D_second_RanSearch.sav'\n",
    "# pickle.dump(dup_fashion_pipes, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dup_fashion_pipes['grid_search'].cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "total = len(grid_dup_fashion_pipes['grid_search'].cv_results_['rank_test_score'])\n",
    "for i in range(total):\n",
    "    if grid_dup_fashion_pipes['grid_search'].cv_results_['rank_test_score'][i] == 1:\n",
    "        results.append(grid_dup_fashion_pipes['grid_search'].cv_results_['params'][i])\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
