{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# General functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, PredefinedSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import sklearn.linear_model\n",
    "# import sklearn.model_selection as skms\n",
    "# import sklearn.feature_selection\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom functions\n",
    "from load_data import load_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and validation data:\n",
    "\n",
    "x data is an array of N*784 pixels (N = 2102 for tr, 600 for va)\n",
    "\n",
    "y is a dataframe of index, class_name and class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of train_x.csv: arr of shape (2102, 784)\n",
      "Contents of valid_x.csv: arr of shape (600, 784)\n"
     ]
    }
   ],
   "source": [
    "x_tr, y_tr_df = load_data('x_train.csv', 'y_train.csv')\n",
    "x_va, y_va_df = load_data('x_valid.csv', 'y_valid.csv')\n",
    "x_te = load_data('x_test.csv', 'y_valid.csv')[0]\n",
    "\n",
    "\n",
    "for label, arr in [('train', x_tr), ('valid', x_va)]:\n",
    "    print(\"Contents of %s_x.csv: arr of shape %s\" % (\n",
    "        label, str(arr.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a random image from the validation data for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting index for each class in training set:\n",
    "top_int = y_tr_df.index[y_tr_df['class_name']=='top']    #index 1\n",
    "trous_int = y_tr_df.index[y_tr_df['class_name']=='trouser']    #index 11\n",
    "dress_int =(y_tr_df.index[y_tr_df['class_name']=='dress']).tolist()\n",
    "pull_int =(y_tr_df.index[y_tr_df['class_name']=='pullover']).tolist()\n",
    "sneaker_int =(y_tr_df.index[y_tr_df['class_name']=='sneaker']).tolist()\n",
    "sandal_int =(y_tr_df.index[y_tr_df['class_name']=='sandal']).tolist()\n",
    "\n",
    "\n",
    "#sneakers + sandals + tops + trous + dress + pullovers in new dataset.\n",
    "train_x_2 = np.vstack((x_tr[sandal_int, :], x_tr[sneaker_int, :]))\n",
    "tops800= np.tile(x_tr[top_int, :], (800, 1)) #make 800 copies of tops\n",
    "train_x_2 = np.vstack((train_x_2, tops800)) # add 800 tops to sandals + sneakers\n",
    "assert(train_x_2.shape == (2400, 784))  #checking dimensions\n",
    "\n",
    "trous800 = np.tile(x_tr[trous_int, :], (800, 1))  #800 copies of trousers\n",
    "train_x_2 = np.vstack((train_x_2, trous800)) # add 800 trousers\n",
    "assert(train_x_2.shape == (3200, 784))  #checking dimensions\n",
    "\n",
    "dress800 = np.tile(x_tr[dress_int, :], (2, 1))   #doubled dresses 400 to 800\n",
    "assert(dress800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, dress800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4000, 784))  #checking dimensions\n",
    "\n",
    "pull800 = np.tile(x_tr[pull_int, :], (8, 1))\n",
    "assert(pull800.shape == (800, 784))\n",
    "train_x_2 = np.vstack((train_x_2, pull800)) # add 800 dresses\n",
    "assert(train_x_2.shape == (4800, 784))\n",
    "\n",
    "#making train_y_2\n",
    "train_y_2 = np.tile(5, 800)\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(7, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(0, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(1, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(3, 800)))\n",
    "train_y_2 = np.hstack((train_y_2, np.tile(2, 800)))\n",
    "\n",
    "assert(train_y_2.shape == (4800, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ll, label in enumerate([0, 1, 2, 3, 5, 7]):\n",
    "#     match_df = y2_df.query(\"class_id == '%i'\" % label)\n",
    "#     print(ll)\n",
    "#     print(\"class_id == '%i'\" % label)\n",
    "\n",
    "# display(match_df)\n",
    "\n",
    "# y2_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prng = np.random.RandomState(0)\n",
    "# prng = np.random.RandomState()\n",
    "# N = 3 # num examples of each class to show\n",
    "# fig, axgrid = plt.subplots(N, 6, figsize=(6*3, N*2.5))\n",
    "\n",
    "# for ll, label in enumerate(['0', '1', '2', '3', '5', '7']):\n",
    "#     match_df = y2_df.query(\"class_id == '%i'\" % label)\n",
    "#     match_ids_N = prng.choice(match_df.index, size=N)        \n",
    "#     for ii, row_id in enumerate(match_ids_N):\n",
    "#         ax = axgrid[ii, ll]\n",
    "#         x_SS = x_va[row_id].reshape((28,28))\n",
    "#         ax.imshow(x_SS, vmin=0, vmax=255, cmap='gray')\n",
    "#         ax.set_xticks([]); ax.set_yticks([]);\n",
    "#         if ii == 0:\n",
    "#             ax.set_title(label, fontsize=16)\n",
    "# plt.subplots_adjust(left=0.01, right=0.99, wspace=.2, hspace=.01)\n",
    "# plt.tight_layout();\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leigh\\OneDrive - Tufts\\cs135\\projectB\\problem1DFinal.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leigh/OneDrive%20-%20Tufts/cs135/projectB/problem1DFinal.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Combine training and validation datasets\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/leigh/OneDrive%20-%20Tufts/cs135/projectB/problem1DFinal.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x_dup_all \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((train_x_2, x_va))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leigh/OneDrive%20-%20Tufts/cs135/projectB/problem1DFinal.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_dup_all \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((train_y_2, y_va_df[\u001b[39m'\u001b[39m\u001b[39mclass_uid\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leigh/OneDrive%20-%20Tufts/cs135/projectB/problem1DFinal.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining X shape: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mValidation X shape: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mCombined X shape: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (train_x_2\u001b[39m.\u001b[39mshape, x_va\u001b[39m.\u001b[39mshape, x_dup_all\u001b[39m.\u001b[39mshape))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine training and validation datasets\n",
    "x_dup_all = np.vstack((train_x_2, x_va))\n",
    "y_dup_all = np.hstack((train_y_2, y_va_df['class_uid']))\n",
    "\n",
    "print(\"Training X shape: %s\\nValidation X shape: %s\\nCombined X shape: %s\\n\" % (train_x_2.shape, x_va.shape, x_dup_all.shape))\n",
    "print(\"Training Y shape: %s\\nValidation Y shape: %s\\nCombined Y shape: %s\\n\" % (train_y_2.shape, y_va_df['class_uid'].shape, y_dup_all.shape))\n",
    "\n",
    "valid_dup_indicators = np.hstack([\n",
    "    -1 * np.ones(train_y_2.shape[0]), #-1 = exclude this example in test split\n",
    "    0 * np.ones(y_va_df.shape[0]), #0 = include in the first test split\n",
    "    ])\n",
    "\n",
    "# Define custom splitter to use only the validation dataset for hyperparameter selection\n",
    "print(\"Splitter dimensions: %i\" % (valid_dup_indicators.shape[0]))\n",
    "my_dup_splitter = sklearn.model_selection.PredefinedSplit(valid_dup_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_class_dist = y_dup_all['class_name'].value_counts()\n",
    "# val_class_dist = y_va_df['class_name'].value_counts()\n",
    "\n",
    "# print('Training class distribution:\\n' + str(tr_class_dist))\n",
    "# print('Validation class distribution:\\n' + str(val_class_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is heavily skewed with data from sandals and trainers, with only one training image for 2 classes. A challenge will be gaining a balanced weighting for each of the classes such that the dominant classes aren't always favoured by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run search with predefined split s.t. validation set is used for hyperparameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid search as per Preetish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed following parameters that don't apply to lbfgs:\n",
    "# batch size only for non-lbfgs. \n",
    "# Learning Rate = const, adaptive, etc... ONLY for sgd\n",
    "# learning_rate_init only used for sgd or adam\n",
    "# early_stopping, n_iter_no_change and validation_fraction only for sgd/adam.\n",
    "\n",
    "scaling = False\n",
    "flag = 'stop'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_coarse_grid_search.sav'\n",
    "else:\n",
    "    filename = '1D_coarse_grid_searchNoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    grid_1D_model = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    rand_param_dist = dict(hidden_layer_sizes=[(10,),(20,),(50,),(100,)], activation=('identity', 'relu'), max_iter = np.logspace(1,2.6,6,dtype=int), alpha = np.logspace(-5,3,9))\n",
    "    \n",
    "    if scaling:\n",
    "        grid_1D_model =   sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLPClassifier(solver='lbfgs', random_state=0), rand_param_dist, scoring='balanced_accuracy', error_score='raise', return_train_score=True, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "        ])\n",
    "    else:\n",
    "        grid_1D_model =   sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLPClassifier(solver='lbfgs', random_state=0), rand_param_dist, scoring='balanced_accuracy', error_score='raise', return_train_score=True, cv= my_dup_splitter, n_jobs = -1, refit= False))\n",
    "        ])\n",
    "\n",
    "    # Fit on x_all as the custom splitter will divide this into tr and val\n",
    "    grid_1D_model.fit(x_dup_all, y_dup_all)\n",
    "    pickle.dump(grid_1D_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7600000000000001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1DRes = grid_1D_model['grid_search']\n",
    "grid1DRes.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract best values from grid search to make best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 418 with Balanced Accuracy: 0.760000\n",
      "Alpha: 1000.000000\n",
      "Layer: (20,)\n",
      "Activation: relu\n",
      "Max_iter: 190\n"
     ]
    }
   ],
   "source": [
    "# When refit is true we can use the best_estimator_ method, but this doesn't work with refit=false - here we need to manually run a fit on our training set.\n",
    "gridRes = grid_1D_model['grid_search'].cv_results_\n",
    "bestIdx = grid_1D_model['grid_search'].best_index_ \n",
    "\n",
    "testScore = gridRes['mean_test_score']\n",
    "\n",
    "# print(testScore)\n",
    "# print(gridRes['params'][bestIdx])\n",
    "\n",
    "bestAlpha = gridRes['params'][bestIdx]['alpha']\n",
    "bestLayer = gridRes['params'][bestIdx]['hidden_layer_sizes']\n",
    "bestActivation = gridRes['params'][bestIdx]['activation']\n",
    "bestMaxIter = gridRes['params'][bestIdx]['max_iter']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx,testScore[bestIdx],bestAlpha,str(bestLayer),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.666250\n",
      "Validation balanced accuracy: 0.638333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# Model with params from best index\n",
    "if scaling:\n",
    "        bestMLP1Dgrid =   sklearn.pipeline.Pipeline([\n",
    "        ('scaling', MinMaxScaler()),\n",
    "        ('MLP', MLPClassifier(solver='lbfgs', random_state=0, activation=bestActivation, alpha=bestAlpha, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter))])\n",
    "else:\n",
    "        bestMLP1Dgrid =   sklearn.pipeline.Pipeline([\n",
    "        ('MLP', MLPClassifier(solver='lbfgs', random_state=0, activation=bestActivation, alpha=bestAlpha, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter))])\n",
    "\n",
    "bestMLP1Dgrid.fit(x_tr,y_tr_df[\"class_name\"])\n",
    "\n",
    "pred_tr = bestMLP1Dgrid.predict(x_tr)\n",
    "pred_va = bestMLP1Dgrid.predict(x_va)\n",
    "pred_te = bestMLP1Dgrid.predict(x_te)\n",
    "\n",
    "tr_acc = balanced_accuracy_score(y_tr_df['class_name'], pred_tr)\n",
    "va_acc = balanced_accuracy_score(y_va_df['class_name'], pred_va)\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))\n",
    "\n",
    "\n",
    "# # # Save output of prediction on test data to a file.\n",
    "# np.savetxt('yhat_test.txt', pred_te, delimiter='\\n', fmt='%s');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate alpha - other values are from best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 260 with Balanced Accuracy: 0.803333\n",
      "Alpha: 100000.000000\n",
      "Layer: (20,)\n",
      "Activation: relu\n",
      "Max_iter: 190\n"
     ]
    }
   ],
   "source": [
    "flag = 'stop'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_alphaGrid_search.sav'\n",
    "else:\n",
    "    filename = '1D_alphaGrid_NoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    evalHypeParamAlphaPipe = pickle.load(open(filename, 'rb'))\n",
    "    alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "    alphaDist = alphaRes['param_alpha']\n",
    "else:\n",
    "    ### Evaluate alpha!\n",
    "\n",
    "    # Create a MLP classifier\n",
    "    MLP1D_EvalHype =   MLPClassifier(solver='lbfgs', activation=bestActivation, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter)\n",
    "    # Hyperparameters distributions - regularization strength alpha\n",
    "\n",
    "    alphaDist = np.logspace(-3,5,17)\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(alpha = alphaDist, random_state=randStateDist) #[10**(-4), optParams['C'], 10**6])\n",
    "\n",
    "    #Pipeline starts!\n",
    "    if scaling:\n",
    "        evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamAlphaPipe.fit(x_dup_all, y_dup_all)    \n",
    "    pickle.dump(evalHypeParamAlphaPipe, open(filename, 'wb'))\n",
    "\n",
    "    alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "bestIdx2 = evalHypeParamAlphaPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = alphaRes['mean_test_score']\n",
    "\n",
    "bestAlpha2 = alphaRes['params'][bestIdx2]['alpha']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx2,testScore[bestIdx2],bestAlpha2,str(bestLayer),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                   0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794,\n",
       "                   0.0031622776601683794, 0.0031622776601683794, 0.01,\n",
       "                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                   0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379,\n",
       "                   0.03162277660168379, 0.03162277660168379, 0.1, 0.1,\n",
       "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                   0.1, 0.1, 0.1, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 0.31622776601683794,\n",
       "                   0.31622776601683794, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795,\n",
       "                   3.1622776601683795, 3.1622776601683795, 10.0, 10.0,\n",
       "                   10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
       "                   10.0, 10.0, 10.0, 10.0, 10.0, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 31.622776601683793,\n",
       "                   31.622776601683793, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
       "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
       "                   100.0, 100.0, 100.0, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 316.22776601683796,\n",
       "                   316.22776601683796, 1000.0, 1000.0, 1000.0, 1000.0,\n",
       "                   1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0,\n",
       "                   1000.0, 1000.0, 1000.0, 1000.0, 1000.0,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795,\n",
       "                   3162.2776601683795, 3162.2776601683795, 10000.0,\n",
       "                   10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0,\n",
       "                   10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0,\n",
       "                   10000.0, 10000.0, 10000.0, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 31622.776601683792,\n",
       "                   31622.776601683792, 100000.0, 100000.0, 100000.0,\n",
       "                   100000.0, 100000.0, 100000.0, 100000.0, 100000.0,\n",
       "                   100000.0, 100000.0, 100000.0, 100000.0, 100000.0,\n",
       "                   100000.0, 100000.0, 100000.0],\n",
       "             mask=[False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False],\n",
       "       fill_value='?',\n",
       "            dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate num units in layer - other values are from best estimator and previous alpha search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 52 with Balanced Accuracy: 0.811667\n",
      "Alpha: 100000.000000\n",
      "Layer: (10,)\n",
      "Activation: relu\n",
      "Max_iter: 190\n"
     ]
    }
   ],
   "source": [
    "flag = 'stop'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_layerGrid_search.sav'\n",
    "else:\n",
    "    filename = '1D_layerGrid_NoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    evalHypeParamLayerPipe = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    ### Evaluate Number of Units!\n",
    "\n",
    "    # Create a MLP classifier\n",
    "    MLP1D_EvalHype = MLPClassifier(solver='lbfgs', activation=bestActivation, alpha=bestAlpha2, max_iter=bestMaxIter)\n",
    "    # Hyperparameters distributions - regularization strength alpha\n",
    "\n",
    "    layerDist = [(1,),(6,),(8,),(10,),(12,),(15,),(20,),(24,),(30,),(40,),(50,),(75,),(100,)]\n",
    "\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(hidden_layer_sizes = layerDist, random_state=randStateDist)\n",
    "\n",
    "    #Pipeline starts!\n",
    "    if scaling:\n",
    "        evalHypeParamLayerPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamLayerPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamLayerPipe.fit(x_dup_all, y_dup_all)\n",
    "    pickle.dump(evalHypeParamLayerPipe, open(filename, 'wb'))\n",
    "\n",
    "layerRes = evalHypeParamLayerPipe['grid_search'].cv_results_\n",
    "bestIdx3 = evalHypeParamLayerPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = layerRes['mean_test_score']\n",
    "\n",
    "bestLayer3 = layerRes['params'][bestIdx3]['hidden_layer_sizes']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx3,testScore[bestIdx3],bestAlpha2,str(bestLayer3),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate max iter - other values are from best estimator and previous alpha and layer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search results:\n",
      "Best index 145 with Balanced Accuracy: 0.836667\n",
      "Alpha: 100000.000000\n",
      "Layer: (10,)\n",
      "Activation: relu\n",
      "Max_iter: 46\n",
      "Rand state: 1\n"
     ]
    }
   ],
   "source": [
    "flag = 'run'\n",
    "\n",
    "if scaling:\n",
    "    filename = '1D_iterGrid_search.sav'\n",
    "else:\n",
    "    filename = '1D_iterGrid_NoScaling.sav'\n",
    "\n",
    "if os.path.isfile(\"./\" + filename) and flag != 'run':\n",
    "    evalHypeParamIterPipe = pickle.load(open(filename, 'rb'))\n",
    "else:\n",
    "    ### Evaluate Max_iter (i.e. early stopping for lbfgs)!\n",
    "\n",
    "    # Create a MLP classifier\n",
    "    MLP1D_EvalHype = MLPClassifier(solver='lbfgs', activation=bestActivation, alpha=bestAlpha2, hidden_layer_sizes = bestLayer3)\n",
    "    # Hyperparameters distributions - max iterations\n",
    "    max_iterDist = np.logspace(0,2.6,15,dtype=int)\n",
    "\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(max_iter = max_iterDist, random_state=randStateDist)\n",
    "\n",
    "    #Pipeline starts!\n",
    "    if scaling:\n",
    "        evalHypeParamIterPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamIterPipe =  sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = -1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamIterPipe.fit(x_dup_all, y_dup_all)\n",
    "    pickle.dump(evalHypeParamIterPipe, open(filename, 'wb'))\n",
    "\n",
    "iterRes = evalHypeParamIterPipe['grid_search'].cv_results_\n",
    "bestIdx4 = evalHypeParamIterPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = iterRes['mean_test_score']\n",
    "bestIter4 = iterRes['params'][bestIdx4]['max_iter']\n",
    "randState4 = iterRes['params'][bestIdx4]['random_state']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\\nRand state: %i\" % (bestIdx4,testScore[bestIdx4],bestAlpha2,str(bestLayer3),bestActivation,bestIter4,randState4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.492083\n",
      "Validation balanced accuracy: 0.478333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# Create the final Best MLP classifier for 1D\n",
    "MLP1D_Final = MLPClassifier(solver='lbfgs', activation=bestActivation, alpha=bestAlpha2, hidden_layer_sizes = bestLayer3, max_iter=bestIter4, random_state=randState4)\n",
    "\n",
    "MLP1D_Final.fit(x_tr,y_tr_df['class_uid'])\n",
    "\n",
    "pred_tr = MLP1D_Final.predict(xsn_tr)\n",
    "pred_va = MLP1D_Final.predict(x_va)\n",
    "pred_te = MLP1D_Final.predict(x_te)\n",
    "\n",
    "tr_acc = balanced_accuracy_score(y_tr_df['class_uid'], pred_tr)\n",
    "va_acc = balanced_accuracy_score(y_va_df['class_uid'], pred_va)\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leigh can you check the stuff between Preetish and here please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 272 into shape (272,16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\avtar\\OneDrive - Tufts\\Tufts CS\\CS135 Intro to ML\\Project B\\CS135-Project-B\\problem1DFinal.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avtar/OneDrive%20-%20Tufts/Tufts%20CS/CS135%20Intro%20to%20ML/Project%20B/CS135-Project-B/problem1DFinal.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Extract Alpha data for plotting\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avtar/OneDrive%20-%20Tufts/Tufts%20CS/CS135%20Intro%20to%20ML/Project%20B/CS135-Project-B/problem1DFinal.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avtar/OneDrive%20-%20Tufts/Tufts%20CS/CS135%20Intro%20to%20ML/Project%20B/CS135-Project-B/problem1DFinal.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# reshape to rows of one alpha, with columns of different random seeds\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/avtar/OneDrive%20-%20Tufts/Tufts%20CS/CS135%20Intro%20to%20ML/Project%20B/CS135-Project-B/problem1DFinal.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m alphaScoreTrainReshape \u001b[39m=\u001b[39m alphaRes[\u001b[39m'\u001b[39;49m\u001b[39mmean_train_score\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape((\u001b[39mlen\u001b[39;49m(alphaDist),\u001b[39m16\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avtar/OneDrive%20-%20Tufts/Tufts%20CS/CS135%20Intro%20to%20ML/Project%20B/CS135-Project-B/problem1DFinal.ipynb#Y103sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m alphaScoreValReshape \u001b[39m=\u001b[39m alphaRes[\u001b[39m'\u001b[39m\u001b[39mmean_test_score\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mreshape((\u001b[39mlen\u001b[39m(alphaDist),\u001b[39m16\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/avtar/OneDrive%20-%20Tufts/Tufts%20CS/CS135%20Intro%20to%20ML/Project%20B/CS135-Project-B/problem1DFinal.ipynb#Y103sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Select best runs for each alpha\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 272 into shape (272,16)"
     ]
    }
   ],
   "source": [
    "# Extract Alpha data for plotting\n",
    "# alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "# reshape to rows of one alpha, with columns of different random seeds\n",
    "alphaScoreTrainReshape = alphaRes['mean_train_score'].reshape((len(alphaDist),16))\n",
    "alphaScoreValReshape = alphaRes['mean_test_score'].reshape((len(alphaDist),16))\n",
    "\n",
    "# Select best runs for each alpha\n",
    "alpha_Train = np.max(alphaScoreTrainReshape,axis=1)\n",
    "alpha_Val = np.max(alphaScoreValReshape,axis=1)\n",
    "# Make second alphaDist to be used for scatter plot (i.e. values repeated for each rand state)\n",
    "alphaDistRep = alphaRes['param_alpha']\n",
    "\n",
    "# Extract layer data for plotting\n",
    "layerRes = evalHypeParamLayerPipe['grid_search'].cv_results_\n",
    "# reshape to rows of one layer size, with columns of different random seeds\n",
    "layerScoreTrainReshape = layerRes['mean_train_score'].reshape((len(layerDist),16))\n",
    "layerScoreValReshape = layerRes['mean_test_score'].reshape((len(layerDist),16))\n",
    "\n",
    "# Select best runs for each layer size\n",
    "layer_Train = np.max(layerScoreTrainReshape,axis=1)\n",
    "layer_Val = np.max(layerScoreValReshape,axis=1)\n",
    "# Make second layerDist to be used for scatter plot (i.e. values repeated for each rand state)\n",
    "layerDistUnpack = [e for e, in layerDist]\n",
    "layerDistRep = np.repeat(layerDistUnpack,16)\n",
    "\n",
    "# # Extract max_iter data for plotting\n",
    "# IterRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "# reshape to rows of one max_iter, with columns of different random seeds\n",
    "iterScoreTrainReshape = iterRes['mean_train_score'].reshape((len(max_iterDist),16))\n",
    "iterScoreValReshape = iterRes['mean_test_score'].reshape((len(max_iterDist),16))\n",
    "\n",
    "# Select best runs for each iter\n",
    "iter_Train = np.max(iterScoreTrainReshape,axis=1)\n",
    "iter_Val = np.max(iterScoreValReshape,axis=1)\n",
    "# Make second iterDist to be used for scatter plot (i.e. values repeated for each rand state)\n",
    "iterDistRep = iterRes['param_max_iter']\n",
    "\n",
    "\n",
    "# Plot accuracy vs alpha, layer and max_iter hyperparameters\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(10,12))\n",
    "ax1.scatter(alphaDistRep, alphaRes['mean_train_score'], label='Training Balanced Accuracy',marker='.')\n",
    "ax1.scatter(alphaDistRep, alphaRes['mean_test_score'], label='Validation Balanced Accuracy',marker='.')\n",
    "ax1.plot(alphaDist, alpha_Train, label='Best Training Balanced Accuracy')\n",
    "ax1.plot(alphaDist, alpha_Val, label='Best Validation Balanced Accuracy')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Alpha Penalization')\n",
    "ax1.set_ylabel('Balanced Accuracy')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(which='both')\n",
    "\n",
    "ax2.scatter(layerDistRep, layerRes['mean_train_score'], label='Training Balanced Accuracy',marker='.')\n",
    "ax2.scatter(layerDistRep, layerRes['mean_test_score'], label='Validation Balanced Accuracy',marker='.')\n",
    "ax2.plot(layerDistUnpack, layer_Train, label='Best Training Balanced Accuracy')\n",
    "ax2.plot(layerDistUnpack, layer_Val, label='Best Validation Balanced Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Units in Hidden Layer')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(which='both')\n",
    "\n",
    "ax3.scatter(iterDistRep, iterRes['mean_train_score'], label='Training Balanced Accuracy',marker='.')\n",
    "ax3.scatter(iterDistRep, iterRes['mean_test_score'], label='Validation Balanced Accuracy',marker='.')\n",
    "ax3.plot(max_iterDist, iter_Train, label='Best Training Balanced Accuracy')\n",
    "ax3.plot(max_iterDist, iter_Val, label='Best Validation Balanced Accuracy')\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('Max Iter')\n",
    "ax3.set_ylabel('Balanced Accuracy')\n",
    "ax3.set_xscale('log')\n",
    "ax3.grid(which='both')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.ConfusionMatrixDisplay.from_estimator(evalHypeParamAlphaPipe['class_uid'], x_va, y_va_df['class_uid'])\n",
    "\n",
    "#id_label = dict(labels = {'dress', 'pullover', 'top', 'trouser', 'sandal', 'sneaker'}, class_id = {3, 2, 0, 1, 5, 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test parameters from Mike's slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1903063129.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    alphaDist = np.logspace(-1.5,5,17)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "MLP1D_EvalHype =   MLPClassifier(solver='lbfgs', activation=bestActivation, hidden_layer_sizes=bestLayer, max_iter=bestMaxIter)\n",
    "    # Hyperparameters distributions - regularization strength alpha\n",
    "\n",
    "    alphaDist = np.logspace(-1.5,5,17)\n",
    "    randStateDist = range(0,16,1)\n",
    "    distEvalHype = dict(alpha = alphaDist, random_state=randStateDist) #[10**(-4), optParams['C'], 10**6])\n",
    "\n",
    "    #Pipeline starts!\n",
    "    if scaling:\n",
    "        evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "            ('scaling', MinMaxScaler()),\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "        ])\n",
    "    else:\n",
    "        evalHypeParamAlphaPipe = sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=my_dup_splitter, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= False, n_jobs = 1))\n",
    "        ])\n",
    "\n",
    "    evalHypeParamAlphaPipe.fit(x_dup_all, y_dup_all)    \n",
    "    pickle.dump(evalHypeParamAlphaPipe, open(filename, 'wb'))\n",
    "\n",
    "alphaRes = evalHypeParamAlphaPipe['grid_search'].cv_results_\n",
    "bestIdx2 = evalHypeParamAlphaPipe['grid_search'].best_index_ \n",
    "\n",
    "testScore = alphaRes['mean_test_score']\n",
    "\n",
    "bestAlpha2 = alphaRes['params'][bestIdx2]['alpha']\n",
    "\n",
    "print(\"Grid search results:\\nBest index %i with Balanced Accuracy: %f\\nAlpha: %f\\nLayer: %s\\nActivation: %s\\nMax_iter: %i\" % (bestIdx2,testScore[bestIdx2],bestAlpha2,str(bestLayer),bestActivation,bestMaxIter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.920625\n",
      "Validation balanced accuracy: 0.836667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avtar\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "distEvalHype = dict(alpha = [bestAlpha2], random_state=[randState4])\n",
    "MLP1D_EvalHype =   MLPClassifier(solver='lbfgs', activation=bestActivation, hidden_layer_sizes=bestLayer3, max_iter=bestIter4)\n",
    "\n",
    "\n",
    "evalHypeParamPipe = sklearn.pipeline.Pipeline([\n",
    "            ('grid_search', GridSearchCV(MLP1D_EvalHype, distEvalHype, cv=5, verbose=0, error_score='raise', scoring='balanced_accuracy', return_train_score=True, refit= True, n_jobs = 1))\n",
    "        ])\n",
    "\n",
    "evalHypeParamPipe.fit(train_x_2,train_y_2)\n",
    "\n",
    "bestE = evalHypeParamPipe['grid_search'].best_estimator_\n",
    "pred_tr = bestE.predict(x_tr)\n",
    "pred_va = bestE.predict(x_va)\n",
    "pred_te = bestE.predict(x_te)\n",
    "\n",
    "tr_acc = balanced_accuracy_score(y_tr_df['class_uid'], pred_tr)\n",
    "va_acc = balanced_accuracy_score(y_va_df['class_uid'], pred_va)\n",
    "\n",
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training balanced accuracy: 0.920625\n",
      "Validation balanced accuracy: 0.836667\n"
     ]
    }
   ],
   "source": [
    "print(\"Training balanced accuracy: %f\\nValidation balanced accuracy: %f\" % (tr_acc, va_acc))\n",
    "\n",
    "# np.savetxt('yhat_test.txt', pred_te, delimiter='\\n', fmt='%s')\n",
    "\n",
    "save_text(pred_te,'yhat_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_text(y_hat, file_name):\n",
    "    names = ['top', 'trouser','pullover', 'dress','fifth','sandal','sixth','sneaker']\n",
    "    \n",
    "\n",
    "    # Load the dataset of interest\n",
    "    datadir = os.path.abspath('data_fashion')\n",
    "    x_NF = np.loadtxt(\n",
    "        os.path.join(datadir, 'x_valid.csv'),\n",
    "        delimiter=',',\n",
    "        skiprows=1)\n",
    "    N = x_NF.shape[0]\n",
    "\n",
    "    # Create random predictions (just for fun)\n",
    "    prng = np.random.RandomState(100)\n",
    "    predictions = []\n",
    "    for n in range(len(y_hat)):\n",
    "        result = names[int(y_hat[n])]\n",
    "        predictions.append(result)\n",
    "\n",
    "    \n",
    "    # Save the predictions in the leaderboard format\n",
    "    np.savetxt(file_name, predictions, delimiter='\\n', fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
